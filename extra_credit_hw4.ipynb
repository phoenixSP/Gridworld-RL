{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extra_credit_hw4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEkthFS47G8+NyzDPK65qY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phoenixSP/Gridworld-RL/blob/master/extra_credit_hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGNgZ2nGBMWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZAb41x0BeuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXspiJ_yFxwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GridWorld4x4(gym.Env):\n",
        "  metadata = {'render.modes': ['console']}\n",
        "\n",
        "  # Define constants for clearer code\n",
        "  LEFT = 2\n",
        "  RIGHT = 3\n",
        "  UP = 0\n",
        "  DOWN = 1\n",
        "\n",
        "  def __init__(self, start, terminal_win, terminal_lose, obstacles):\n",
        "    super(GridWorld4x4, self).__init__()\n",
        "    self.actions = [\"UP\", \"DOWN\",  \"LEFT\", \"RIGHT\"] #is this required ???\n",
        "    n_actions = 4\n",
        "\n",
        "\n",
        "    self.grid_size = 4\n",
        "    self.start = start\n",
        "    self.agent_pos = self.start\n",
        "    self.obstacles = obstacles\n",
        "    self.determine = False\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    self.terminal_states = [terminal_win, terminal_lose]\n",
        "\n",
        "    self.observation_space = spaces.Box(low = 0, high = 5, shape = (4,4,1), dtype = np.float32)\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "\n",
        "    self.agent_pos = self.start   \n",
        "    return self.generate_state()\n",
        "\n",
        "  def _choose_action_probability(self, action):\n",
        "    if action == 0:\n",
        "        return np.random.choice([\"UP\", \"LEFT\", \"RIGHT\"], p=[0.8, 0.1, 0.1])\n",
        "    if action == 1:\n",
        "        return np.random.choice([\"DOWN\", \"LEFT\", \"RIGHT\"], p=[0.8, 0.1, 0.1])\n",
        "    if action == 2:\n",
        "        return np.random.choice([\"LEFT\", \"UP\", \"DOWN\"], p=[0.8, 0.1, 0.1])\n",
        "    if action == 3:\n",
        "        return np.random.choice([\"RIGHT\", \"UP\", \"DOWN\"], p=[0.8, 0.1, 0.1])\n",
        "\n",
        "  def next_position(self, action):\n",
        "\n",
        "    action = self._choose_action_probability(action)\n",
        "\n",
        "    if action == \"UP\":\n",
        "        nxtState = (self.agent_pos[0] - 1, self.agent_pos[1])\n",
        "    elif action == \"DOWN\":\n",
        "        nxtState = (self.agent_pos[0] + 1, self.agent_pos[1])\n",
        "    elif action == \"LEFT\":\n",
        "        nxtState = (self.agent_pos[0], self.agent_pos[1] - 1)\n",
        "    elif action == \"RIGHT\":\n",
        "        nxtState = (self.agent_pos[0], self.agent_pos[1] + 1)\n",
        "\n",
        "    # if next state is legal\n",
        "    if not self.is_obstacle_or_wall(nxtState):\n",
        "      #print(\"Moving\", nxtState)\n",
        "      return nxtState\n",
        "\n",
        "    #print(\"Not moving\", self.agent_pos)\n",
        "    return self.agent_pos\n",
        "\n",
        "  def is_obstacle_or_wall(self, state):\n",
        "    if state[0] < 0 or state[0] >= self.grid_size:\n",
        "      return True\n",
        "    \n",
        "    if state[1] < 0 or state[1] >= self.grid_size:\n",
        "      return True\n",
        "\n",
        "    if state in self.obstacles:\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "  def is_terminal(self):\n",
        "    \n",
        "    if self.agent_pos in self.terminal_states:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "  def generate_state(self):\n",
        "\n",
        "    grid = np.zeros(( self.grid_size, self.grid_size, 1))\n",
        "    grid[self.agent_pos[0], self.agent_pos[1], 0] = 1\n",
        "\n",
        "    for obs in self.obstacles:\n",
        "      grid[obs[0], obs[1], 0] = 2\n",
        "\n",
        "    for i, state in enumerate(self.terminal_states):\n",
        "      if i == 0:\n",
        "        grid[state[0], state[1], 0] = 5\n",
        "      else:\n",
        "        grid[state[0], state[1], 0] = 4\n",
        "    \n",
        "    return grid.astype(np.float32)\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    self.agent_pos = self.next_position(action)\n",
        "    next_state = self.generate_state()\n",
        "\n",
        "    done = self.is_terminal()\n",
        "\n",
        "    info = {}\n",
        "    goal_reward = 1\n",
        "    lose_reward = -1\n",
        "    step_reward = -0.1\n",
        "    if self.agent_pos == self.terminal_states[0]:\n",
        "      reward = goal_reward\n",
        "    elif self.agent_pos == self.terminal_states[1]:\n",
        "      reward = lose_reward\n",
        "    else:\n",
        "      reward = step_reward\n",
        "\n",
        "    return next_state, reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "\n",
        "    grid = np.zeros(( self.grid_size, self.grid_size))\n",
        "    for obs in self.obstacles:\n",
        "      grid[obs[0], obs[1]] = 2\n",
        "\n",
        "    for i, state in enumerate(self.terminal_states):\n",
        "      if i == 0:\n",
        "        grid[state[0], state[1]] = 5\n",
        "      else:\n",
        "        grid[state[0], state[1]] = 4\n",
        "    grid[self.agent_pos[0], self.agent_pos[1]] = 1\n",
        "    print(\"Agent's position:\", self.agent_pos[0], self.agent_pos[1])\n",
        "    print(grid)\n",
        "\n",
        "  def close(self):\n",
        "    pass  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Cvrypq1Dl3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "db9a5420-9681-4854-f549-8c0ba21e8b30"
      },
      "source": [
        "from stable_baselines.common.env_checker import check_env"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th7zLtCb1DY7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a704b831-ccf4-46b2-e1ee-e35aa2a266f3"
      },
      "source": [
        "start = (3,0)\n",
        "terminal_win = (2,3)\n",
        "terminal_lose = (1,3)\n",
        "obstacles = [(1,1)]\n",
        "env = GridWorld4x4(start, terminal_win, terminal_lose, obstacles)\n",
        "\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ia7r5HuoxDL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "b20d7331-48ce-455a-ae8e-6317511842d8"
      },
      "source": [
        "start = (3,0)\n",
        "terminal_win = (2,3)\n",
        "terminal_lose = (1,3)\n",
        "obstacles = [(1,1)]\n",
        "env = GridWorld4x4(start, terminal_win, terminal_lose, obstacles)\n",
        "\n",
        "moves = [3,3,3,0]\n",
        "\n",
        "for move in moves:\n",
        "  print(\"Step {}\".format(move + 1))\n",
        "  obs, reward, done, info = env.step(move)\n",
        "  print('reward=', reward, 'done=', done)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 4\n",
            "Moving (2, 0)\n",
            "reward= 1 done= False\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 2. 0. 4.]\n",
            " [1. 0. 0. 5.]\n",
            " [0. 0. 0. 0.]]\n",
            "Step 4\n",
            "Moving (1, 0)\n",
            "reward= 1 done= False\n",
            "[[0. 0. 0. 0.]\n",
            " [1. 2. 0. 4.]\n",
            " [0. 0. 0. 5.]\n",
            " [0. 0. 0. 0.]]\n",
            "Step 4\n",
            "Not moving (1, 0)\n",
            "reward= 1 done= False\n",
            "[[0. 0. 0. 0.]\n",
            " [1. 2. 0. 4.]\n",
            " [0. 0. 0. 5.]\n",
            " [0. 0. 0. 0.]]\n",
            "Step 1\n",
            "Moving (0, 0)\n",
            "reward= 1 done= False\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 2. 0. 4.]\n",
            " [0. 0. 0. 5.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFC-p_fhsa_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e60d7f0d-4eb7-460b-b19f-0d1c1c45acba"
      },
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "start = (3,0)\n",
        "terminal_win = (2,3)\n",
        "terminal_lose = (1,3)\n",
        "obstacles = [(1,1)]\n",
        "env = GridWorld4x4(start, terminal_win, terminal_lose, obstacles)\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IFQFiWruaal",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "9c8e2d33-d473-49e2-aa32-6eae441c2b31"
      },
      "source": [
        "model = ACKTR('MlpPolicy', env, verbose=1).learn(5000)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| explained_variance | -0.0447  |\n",
            "| fps                | 16       |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 1.39     |\n",
            "| policy_loss        | -1.3     |\n",
            "| total_timesteps    | 20       |\n",
            "| value_loss         | 1.14     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 6.21     |\n",
            "| ep_reward_mean     | 0.279    |\n",
            "| explained_variance | 0.55     |\n",
            "| fps                | 482      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 0.42     |\n",
            "| policy_loss        | -0.0878  |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 0.0455   |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| ep_len_mean        | 5.23      |\n",
            "| ep_reward_mean     | 0.477     |\n",
            "| explained_variance | 0.748     |\n",
            "| fps                | 597       |\n",
            "| nupdates           | 200       |\n",
            "| policy_entropy     | 0.0717    |\n",
            "| policy_loss        | -0.000416 |\n",
            "| total_timesteps    | 4000      |\n",
            "| value_loss         | 0.00908   |\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEGg_EyhumQI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19fdc211-4281-42c7-c21c-6bd895133651"
      },
      "source": [
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('reward=', reward, 'done=', done)\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically when a done signal is encountered, so that final state is not printed\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1\n",
            "Action:  [3]\n",
            "reward= [-0.1] done= [False]\n",
            "Agent's position: 3 1\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 2. 0. 4.]\n",
            " [0. 0. 0. 5.]\n",
            " [0. 1. 0. 0.]]\n",
            "Step 2\n",
            "Action:  [3]\n",
            "reward= [-0.1] done= [False]\n",
            "Agent's position: 3 2\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 2. 0. 4.]\n",
            " [0. 0. 0. 5.]\n",
            " [0. 0. 1. 0.]]\n",
            "Step 3\n",
            "Action:  [3]\n",
            "reward= [-0.1] done= [False]\n",
            "Agent's position: 3 3\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 2. 0. 4.]\n",
            " [0. 0. 0. 5.]\n",
            " [0. 0. 0. 1.]]\n",
            "Step 4\n",
            "Action:  [0]\n",
            "reward= [-0.1] done= [False]\n",
            "Agent's position: 3 2\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 2. 0. 4.]\n",
            " [0. 0. 0. 5.]\n",
            " [0. 0. 1. 0.]]\n",
            "Step 5\n",
            "Action:  [3]\n",
            "reward= [-0.1] done= [False]\n",
            "Agent's position: 3 2\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 2. 0. 4.]\n",
            " [0. 0. 0. 5.]\n",
            " [0. 0. 1. 0.]]\n",
            "Step 6\n",
            "Action:  [3]\n",
            "reward= [-0.1] done= [False]\n",
            "Agent's position: 3 3\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 2. 0. 4.]\n",
            " [0. 0. 0. 5.]\n",
            " [0. 0. 0. 1.]]\n",
            "Step 7\n",
            "Action:  [0]\n",
            "reward= [1.] done= [ True]\n",
            "Agent's position: 3 0\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 2. 0. 4.]\n",
            " [0. 0. 0. 5.]\n",
            " [1. 0. 0. 0.]]\n",
            "Goal reached! reward= [1.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}